input {
  file {
    # lembre-se de editar o caminho para usar seu arquivo stocks.csv
    path => "C:/Temp/Elastic/Python/arq01.csv"
    # O código a seguir assegurará a releitura da entrada completa 
    # cada vez que o Logstash for executado (útil para depuração).
    start_position => "beginning"
    sincedb_path => "NUL"
  }
}
## ETAPA 2
filter {
   csv {
    columns => ["column1","column2","column3"]
    separator => ";"
    convert => {
	 column1 => "float" 
    }
   }

  mutate {
                add_field => { "newtimestamp" => "%{@timestamp}" }
               }
           date {
                match => ["newtimestamp","yyyy-MM-dd'T'HH:mm:ss.SSSZ", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
                #timezone => "Etc/GMT+3"
                target => "@timestamp"
           }

   date {
     match => [ "column2" , "dd/mm/yyyy HH:mm" ]
     timezone => "UTC"   
   }  
}

output {
  # A saída a seguir para stdout é apenas para depuração 
  # e pode ser removida
    elasticsearch {
      index => "idx_speed_%{+YYYY.MM}"
      hosts => "http://localhost:9200"
    }
    
    stdout {}
  }  